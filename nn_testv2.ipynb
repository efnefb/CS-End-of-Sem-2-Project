{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, n0, n1, activation='ReLU', seed=42, leaky_alpha=0):\n",
    "        self.w_shape = (n1, n0)\n",
    "        self.activation = activation\n",
    "        self.leaky_alpha = leaky_alpha\n",
    "        self.seed = seed\n",
    "    \n",
    "    def output(self, input_vector, weight_mat, bias_vec): #Implements the computation at a single layer for 1 training example\n",
    "        linear_combine = np.matmul(weight_mat, input_vector) + np.array(bias_vec) #Linearly combine and add bias\n",
    "        \n",
    "        output_vector = 0\n",
    "\n",
    "        #Activate\n",
    "        if self.activation == 'ReLU': output_vector = np.where(linear_combine>0, linear_combine, self.leaky_alpha * linear_combine)\n",
    "        \n",
    "        elif self.activation== 'Logistic': output_vector = 1 / (1  + np.exp(-1 * np.array(linear_combine)))\n",
    "            \n",
    "        \n",
    "        elif self.activation == 'tanh': output_vector = np.tanh(linear_combine)\n",
    "        elif self.activation == 'arctan': output_vector = np.arctan(linear_combine)\n",
    "        elif self.activation == 'Softmax': output_vector = np.exp(linear_combine) / np.sum(np.exp(linear_combine))\n",
    "        elif self.activation == 'None': output_vector = linear_combine\n",
    "        \n",
    "        return output_vector\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '< ' + str(self.w_shape[1]) + ', ' + str(self.w_shape[0]) + ', ' + self.activation + ' >'\n",
    "\n",
    "class NN():\n",
    "    def __init__(self):\n",
    "        self.layers = [] \n",
    "        \n",
    "        self.num_params = 0\n",
    "        self.allParams = []\n",
    "        \n",
    "    #Implementing the architecture\n",
    "    def add_layer_at(self, layer, idx):\n",
    "        self.layers.insert(idx, layer)\n",
    "        \n",
    "        if idx!=0 and idx!=len(self.layers)-1:\n",
    "            if self.layers[idx-1].w_shape[0] != self.layers[idx].w_shape[1]:\n",
    "                raise IndexError(\"Layer nodes must match\")\n",
    "            if self.layers[idx].w_shape[0] != self.layers[idx+1].w_shape[1]:\n",
    "                raise IndexError(\"Layer nodes must match\")\n",
    "        elif idx==0:\n",
    "            if self.layers[idx].w_shape[0] != self.layers[idx+1].w_shape[1]:\n",
    "                raise IndexError(\"Layer nodes must match\")\n",
    "        else:\n",
    "            if self.layers[idx-1].w_shape[0] != self.layers[idx].w_shape[1]:\n",
    "                raise IndexError(\"Layer nodes must match\")\n",
    "        \n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if len(self.layers) >=2:\n",
    "            if self.layers[-2].w_shape[0] != self.layers[-1].w_shape[1]:\n",
    "                raise IndexError(\"Layer nodes must match\")\n",
    "        \n",
    "    def remove_layer(self, layer_idx):\n",
    "        del self.layers[layer_idx]\n",
    "    \n",
    "        \n",
    "    def initialize(self): #Kaiming He initialization method â€“> initialize one long vector of all the NN's weights and biases\n",
    "        self.num_params = sum([layer.w_shape[0] * (layer.w_shape[1] + 1) for layer in self.layers])\n",
    "        self.allParams[:] = np.zeros(self.num_params)\n",
    "        \n",
    "        self.allParams.insert(0,0)\n",
    "        \n",
    "        border = 1\n",
    "        for layer in self.layers:\n",
    "            np.random.rand(layer.seed)\n",
    "            params_len = layer.w_shape[0] * (layer.w_shape[1] + 1)\n",
    "            self.allParams[border:border+params_len] = np.random.normal(0, 2/layer.w_shape[1], params_len)\n",
    "            for i in range(border,border+params_len):\n",
    "                if i%(layer.w_shape[1]+1) == 0 and i!=0: \n",
    "                    self.allParams[i] = 0.01\n",
    "            border = border + params_len\n",
    "        \n",
    "             \n",
    "    def forward_pass(self, input_vector): #1 forward pass for 1 training example\n",
    "        layer_output = input_vector\n",
    "\n",
    "        border = 1\n",
    "        for layer in self.layers:\n",
    "            np.random.seed(layer.seed)\n",
    "\n",
    "            param_len = layer.w_shape[0] * (layer.w_shape[1] + 1)\n",
    "            x = np.array(self.allParams[border: border+param_len]).reshape(1,-1)[0]\n",
    "            params = x.reshape(layer.w_shape[0],layer.w_shape[1]+1)\n",
    "            \n",
    "            param_mat = params.reshape(layer.w_shape[0], layer.w_shape[1] + 1)\n",
    "            \n",
    "            weights = param_mat[:,:-1]\n",
    "            biases = param_mat[:,-1]\n",
    "\n",
    "            \n",
    "            layer_output = layer.output(layer_output, weights, biases)\n",
    "\n",
    "            border = border + param_len\n",
    "            \n",
    "        return layer_output\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layers)\n",
    "\n",
    "\n",
    "class Optimizer():\n",
    "    def __init__(self, model, regularization='None', alpha=0, gamma=0):\n",
    "        self.model = model\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def train(self, allParams, train_features, targets, Loss, lr=0.01): #Performs 1 backprop step for 1 epoch\n",
    "        if Loss == 'MSE':  \n",
    "            gradient_fn = grad(self.MSE, 0)\n",
    "        elif Loss == 'Cross Entropy Loss':\n",
    "            gradient_fn = grad(self.Cross_Entropy_Loss, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Enter a valid loss function\")\n",
    "                \n",
    "        gradients = gradient_fn(allParams, train_features, targets)\n",
    "        \n",
    "        self.model.allParams -= np.array(lr * gradients)\n",
    "        self.model.allParams = np.array([x._value for x in self.model.allParams])\n",
    "\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def Cross_Entropy_Loss(self, allParams, train_features, targets): #For classification tasks\n",
    "        self.model.allParams[:] = allParams\n",
    "        \n",
    "        total_loss = 0\n",
    "        for train_feature in train_features:\n",
    "            prediction = self.model.forward_pass(train_feature)\n",
    "            cost = np.sum( (-1 * np.log(prediction + 1e-8) * targets).flatten() )\n",
    "            total_loss += cost\n",
    "        avg_loss = (total_loss / len(train_features)) + self.decide_regularization(self.regularization, allParams)\n",
    "        return avg_loss\n",
    "\n",
    "    def MSE(self, allParams, train_features, targets): #For regression tasks\n",
    "        self.model.allParams[:] = allParams\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i in enumerate(train_features):\n",
    "            prediction = self.model.forward_pass(i[1])\n",
    "            cost = (prediction - targets[[i[0]]])**2\n",
    "            total_loss += cost\n",
    "        mse = (total_loss / len(train_features)) + self.decide_regularization(self.regularization, allParams)\n",
    "        return mse\n",
    "\n",
    "    def decide_regularization(self, s, a): #Regularization options \n",
    "        if s == 'None':\n",
    "            return 0\n",
    "        elif s == 'Ridge':\n",
    "            return self.alpha * np.sum(np.array( np.array(a)**2))\n",
    "        elif s == 'Lasso':\n",
    "            return self.alpha * np.sum(np.abs(np.array(a)))\n",
    "        elif s == 'EN':\n",
    "            return self.gamma * self.alpha * np.sum(np.array(a)**2) + (1-self.gamma) * self.alpha * np.sum(np.abs(np.array(a)))\n",
    "        else:\n",
    "            raise ValueError(\"Enter a valid regularization method\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.00946997, -0.00197198, ..., -0.00519003,\n",
       "        0.03643563,  0.01278033])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "images = load_digits()['data'][:10] #an array of 100 flattened images, each image an array of length 64\n",
    "\n",
    "labels = load_digits()['target'][:10] #an array of 100 corresponding labels (digits)\n",
    "labels = np.eye(10)[labels] #One hot encoded (so that it can fit the dimensions of the model)\n",
    "\n",
    "\n",
    "nn = NN()\n",
    "nn.add_layer(Layer(64,50,'ReLU',42))\n",
    "nn.add_layer(Layer(50,50,'Logistic',43))\n",
    "nn.add_layer(Layer(50,50,'ReLU',44))\n",
    "nn.add_layer(Layer(50,10,'Softmax',45))\n",
    "\n",
    "nn.initialize()\n",
    "\n",
    "opt = Optimizer(nn, 'Ridge', alpha=0.01)\n",
    "\n",
    "\n",
    "opt.train(np.array(nn.allParams),images, labels, 'Cross Entropy Loss', lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
